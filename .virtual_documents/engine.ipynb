import numpy as np
import pandas as pd
 
from IPython.display import display, HTML
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.io as pio
 
 
import seaborn as sns
from importlib import reload
import matplotlib.pyplot as plt
import matplotlib
import warnings


# Load data and label

train = pd.read_csv("CMaps/train_FD001.txt", sep=r"\s+", header=None)
test = pd.read_csv("CMaps/test_FD001.txt", sep=r"\s+", header=None)
rul = pd.read_csv("CMaps/RUL_FD001.txt", sep=r"\s+", header=None, names=['RUL'])

train = train.dropna(axis=1, how='all')
test = test.dropna(axis=1, how='all')
rul = rul.iloc[:, 0]

columns = (
    ['engine_id', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)]
    + [f'sensor_measurement_{i}' for i in range(1, 22)]
)

train.columns = columns
test.columns = columns


train.describe().T


# Drop features with constant values

drops = []

for i in range(2, min(20, train.shape[1])):  
    try:
        if train.iloc[:, i].min() == train.iloc[:, i].max(): 
            drops.append(train.columns[i])  
    except Exception as e:  
        print(f"Error processing column {train.columns[i]}: {e}")
        pass

print("Dropped columns:", drops)

train.drop(drops, axis=1, inplace=True)
test.drop(drops, axis=1, inplace=True)



# Drop highly correlated features

cor = train.corr().abs()
u = cor.where(np.triu(np.ones(cor.shape), k=1).astype(bool))
cor_feat = [column for column in u.columns if any(u[column] > 0.95)]
train.drop(cor_feat, axis=1, inplace=True)
test.drop(cor_feat, axis=1, inplace=True)
print(cor_feat)


# Define max life for each engine

train_rul = train.groupby(['engine_id']).agg({'time_in_cycles':'max'})
train_rul.rename(columns={'time_in_cycles':'life'}, inplace=True)
train_rul.head()


test_rul = test.groupby(['engine_id']).agg({'time_in_cycles':'max'})
test_rul.rename(columns={'time_in_cycles':'life'}, inplace=True)
test_rul.head()


# Keep only engines that do well

train = train.merge(train_rul, how='left', on=['engine_id'])
train['RUL'] = train['life'] - train['time_in_cycles']
train.drop(['life'], axis=1, inplace=True)
train['RUL'][train['RUL']>125]=125


f = []
import statsmodels.api as sm

def backward_regression(X, y, initial_list=[], threshold_out=0.05, verbose=True):
    included = list(X.columns)
    while True:
        changed = False
        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()
        pvalues = model.pvalues.iloc[1:]
        worst_pval = pvalues.max()  
        if worst_pval > threshold_out:
            changed = True
            worst_feature = pvalues.idxmax()
            included.remove(worst_feature)
            if verbose:
                print(f"worst_feature : {worst_feature}, {worst_pval} ")
        if not changed:
            break
    f.append(included)
    print(f"\nSelected Features:\n{f[0]}")


X = train.iloc[:,1:-1]
y = train.iloc[:,-1]
backward_regression(X, y)



feature_names = f[0]

import time
model_performance = pd.DataFrame(columns=['r-Squared','RMSE','total time'])
 
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import make_scorer, accuracy_score
 
import sklearn
from sklearn.metrics import mean_squared_error, r2_score
# from sklearn.ensemble import RandomForestRegressor
 
model_performance = pd.DataFrame(columns=['R2','RMSE', 'time to train','time to predict','total time'])
 
 
def R_squared(y_true, y_pred):
    SS_res =  K.sum(K.square(y_true - y_pred))
    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))
    return 1 - SS_res/(SS_tot + K.epsilon())
 
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error


test_cycle = test.groupby(['engine_id']).agg({'time_in_cycles':'max'})
test_cycle.rename(columns={'time_in_cycles':'life'},inplace=True)
test_max = test.merge(test_cycle,how='left',on=['engine_id'])
test_max = test_max[(test_max['time_in_cycles']==test_max['life'])]
test_max.drop(['life'],axis=1,inplace=True)
 
X_train = train[feature_names]
y_train = train.iloc[:,-1]
X_test = test_max[feature_names]
y_test = test_rul.iloc[:,-1]
 
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)


from sklearn.neighbors import KNeighborsRegressor
start = time.time()
model = KNeighborsRegressor(n_neighbors=9).fit(X_train,y_train)
end_train = time.time()
y_predictions = model.predict(X_test) # These are the predictions from the test data.
end_predict = time.time()
 
 
 
model_performance.loc['kNN'] = [model.score(X_test,y_test), 
                                   mean_squared_error(y_test,y_predictions,squared=False),
                                   end_train-start,
                                   end_predict-end_train,
                                   end_predict-start]
 
print('R-squared error: '+ "{:.2%}".format(model.score(X_test,y_test)))
print('Root Mean Squared Error: '+ "{:.2f}".format(mean_squared_error(y_test,y_predictions,squared=False)))



